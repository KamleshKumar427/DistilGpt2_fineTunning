# DistilGpt2_fineTunning

  This GitHub repository hosts a comprehensive project that fine-tunes the DistilGPT-2 model for generating coherent and creative stories from prompts. The repository includes model selection code using Hugging Face's Transformers library, specifying the "distilgpt2" model, and configuring essential hyperparameters for training. 
  The training process is well-documented, with key settings like the number of training epochs, learning rate, batch sizes, and more. Additionally, data preprocessing steps, such as text cleaning and bracket removal, are incorporated, ensuring high-quality input data for the model. 
  This repository provides a complete solution for training and infereing a story generation model.
